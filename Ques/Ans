#ques 1
Ans: From the results, we can see that the classification performance of the classifiers trained for various "k" values shows a trend of overfitting. It gradually shifts towards underfitting with increasing values of k. For instance, when k is small (k=1), the model has a high variance and low bias, leading to overfitting. As the value of k increases, the model's variance keeps decreasing, and its bias increasing, leading to underfitting. 
The optimal or the best performing model was found to be the one with k=30, which had a training accuracy of 83.12% and a test accuracy of 84%. This indicates that the model's performance was good and successfully generalize well to unseen data. However, the test accuracy does not improve significantly after k=30, and the training accuracy starts to decrease. This could be an indication of overfitting. As we increase the value of k, the model becomes more biased and less complex, leading to underfitting.
Therefore, we need to choose an optimal value of k that balances between bias and variance, leading to a better classification performance. The optimal value of k can be found by performing cross-validation.


#ques 2
Ans:Based on the results and the graph, we can see that the performance of the classifier using Manhattan distance is slightly worse than the classifier with the lowest test error rate from Question 1, which used Euclidean distance. The classifier using Manhattan distance has a test error rate of 0.16456, while the classifier with the lowest test error rate in Question 1 had a test error rate of 0.16. 
Additionally, we can observe from the graph plot that the decision boundary is slightly different between the two classifiers. The decision boundary of the classifier using Manhattan distance is more irregular and has more abrupt changes than the decision boundary of the classifier using Euclidean distance. This indicates that the classifier using Euclidean distance may be better at capturing the underlying patterns in the data.


#ques 3
Ans: The plot shows the trend of training and test error rates with respect to the model capacity (1/k). The trend of the training and test error rates with respect to the model capacity can be explained in terms of bias and variance. As the model capacity increases, the training error rate decreases and the test error rate initially decreases. However, it starts to increase soon after. The point where the test error rate starts increasing is the overfitting zone, and the point where the training error rate is high is the underfitting zone. In the overfitting zone, the model becomes too complex and starts fitting the noise in the training data, resulting in poor performance on the test data. In contrast, in the underfitting zone, the model is too simple resulting in high bias as it fails to capture the underlying pattern.
At low model capacity (high values of k), the model is too simple, and both the training and test error rates are high. This indicates high bias in the model, and underfitting, as it cannot capture the underlying pattern in the data. As the model capacity increases (low values of k), the model becomes more complex and can capture more complex patterns in the data, resulting in a decrease in the training error rate. However, if the model becomes too complex, it starts to fit the noise in the training data, resulting in high variance and an increase in the test error rate- indicating overfitting.
The optimal model capacity is the one that strikes a balance between bias and variance and achieves the lowest test error rate. In the given plot, the optimal model capacity is around 1/k = 0.002, which corresponds to k = 500.
